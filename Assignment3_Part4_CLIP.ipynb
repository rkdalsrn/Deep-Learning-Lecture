{"nbformat":4,"nbformat_minor":0,"metadata":{"interpreter":{"hash":"696448c147db95129aac2fd56dc49d38b08c0b6499dfa51590df1ba99082e5b1"},"kernelspec":{"display_name":"Python 3.7.12 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"orig_nbformat":4,"colab":{"name":"Assignment3_Part4_CLIP.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"_E8fbGp4Gp7j"},"source":["# M2177.003100 Deep Learning <br> Assignment #3 Part 4: Transformer for vision and language\n","\n","Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Seungryong Yoo, November 2021.\n","\n","In this problem, we will train CLIP which consists of a pair of image and text encoder, which are trained by self-supervised manner.<br>\n","Basically, the encoders are trained to maximize the similarity between the paired image and text in the common representation space.<br>\n","As the image encoder, we will use ViT (Vision Transformer). The underlying structure of ViT is almost the same as Transformers for NLP. <br>\n","The only difference is that word embedding layer is replaced with patch embedding. <br>\n","As the text encoder, we will use pretrained model (DistillBert) <br>\n","\n","This is about VisionTransformer (ViT) (Dosovitskiy et al., 2020).<br>\n","[https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)\n","\n","This is about CLIP (Radford et al., 2021)<br>\n","[https://openai.com/blog/clip/](https://openai.com/blog/clip/)<br>\n","[https://arxiv.org/pdf/2103.00020.pdf](https://arxiv.org/pdf/2103.00020.pdf)<br>\n","(OpenAI blog post might be enough to understand the model)\n","\n","Original blog post & code <br>\n","[https://github.com/lucidrains/vit-pytorch](https://arxiv.org/pdf/2103.00020.pdf) (ViT) <br>\n","[https://github.com/moein-shariatnia/OpenAI-CLIP](https://github.com/moein-shariatnia/OpenAI-CLIP) (CLIP)\n","\n","\n","That said, you are allowed to copy paste the codes from the original repo.\n","HOWEVER, <font color=red> try to implement the model yourself first </font>, and consider the original source code as a last resort.\n","\n","### Attention Implementation (10 points)\n","1. Write up the code for the TODO part of \"Attention\" class in \"clip_modules.py\".\n","2. You will get a full score if you implement it right.\n","\n","### Codes\n","1. clip_utils.py \n","2. clip_modules.py <br>\n","<br>\n","### Submitting your work:\n","<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n","Once you have done **all Assignment Part 1-4**, run the *CollectSubmission.sh* script with your **Student number** as input argument. <br>\n","This will produce a zipped file called *[Your student number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* 20xx_xxxxx)\n","\n","Now proceed to the code.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HiP8Q6z0HEux","executionInfo":{"status":"ok","timestamp":1638297036472,"user_tz":-540,"elapsed":36574,"user":{"displayName":"­강민구 / 학생 / 전기·정보공학부","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010266013705616084"}},"outputId":"402ce979-3ee4-4a0c-8126-f50a59f028ff"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BKygqCuHHGIS","executionInfo":{"status":"ok","timestamp":1638297076145,"user_tz":-540,"elapsed":559,"user":{"displayName":"­강민구 / 학생 / 전기·정보공학부","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010266013705616084"}},"outputId":"796b2a07-4ca5-4115-f855-90ddd2757b74"},"source":["%cd drive/MyDrive/fastMRI_ming/Assignment3"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/fastMRI_ming/Assignment3\n"]}]},{"cell_type":"markdown","metadata":{"id":"zUs8Pxd6Gp7l"},"source":["## Install libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UqNQ3JzwGp7m","executionInfo":{"status":"ok","timestamp":1638297093586,"user_tz":-540,"elapsed":14612,"user":{"displayName":"­강민구 / 학생 / 전기·정보공학부","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010266013705616084"}},"outputId":"f851949f-c298-47ba-d306-0e3cc4e21940"},"source":["!python3 -m pip install pandas\n","!python3 -m pip install einops\n","!python3 -m pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Collecting einops\n","  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.3.2\n","Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 29.9 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 58.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 6.4 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 36.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"]}]},{"cell_type":"code","metadata":{"id":"qeKWtk-KGp7n"},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from transformers import DistilBertTokenizer\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import itertools\n","from tqdm.autonotebook import tqdm\n","from PIL import Image\n","from clip_utils import caption_to_csv, get_transforms, get_lr, AvgMeter, make_train_valid_dfs\n","\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q6sWbqcJGp7n"},"source":["## Preparing dataset"]},{"cell_type":"markdown","metadata":{"id":"mqDMnDJ7Gp7o"},"source":["link : [https://www.kaggle.com/adityajn105/flickr8k](https://www.kaggle.com/adityajn105/flickr8k)\n","\n","1. Download the dataset from attached link.\n","\n","2. Move the downloaded zip file under the \"data\" directory and then unzip the zip file.\n","3. Run the following cell"]},{"cell_type":"code","metadata":{"id":"Ll4vcA9pGp7o"},"source":["# if you successfully run this cell once, do not run this cell again\n","if not os.path.exists('./data/Flicker-8k'):\n","    os.mkdir('./data/Flicker-8k/')\n","\n","os.system('mv ./data/{} ./data/{} ./data/Flicker-8k/'.format('Images', 'captions.txt'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Up6izcoGp7p"},"source":["# convert captions.txt to csv file\n","# result location : ./data/Flicker-8k/captions.csv\n","caption_to_csv()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"osEt3NoBGp7q"},"source":["## Configuration"]},{"cell_type":"code","metadata":{"id":"Wlz1HKU6Gp7q"},"source":["class CFG:\n","    debug = False\n","    image_path = \"./data/Flicker-8k/Images\"\n","    captions_path = \"./data/Flicker-8k\"\n","    batch_size = 32\n","    num_workers = 4\n","    head_lr = 1e-3\n","    image_encoder_lr = 1e-4\n","    text_encoder_lr = 1e-5\n","    weight_decay = 1e-3\n","    patience = 1\n","    factor = 0.8\n","    epochs = 5\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    image_embedding = 2048\n","    text_encoder_model = \"distilbert-base-uncased\"\n","    text_embedding = 768\n","    text_tokenizer = \"distilbert-base-uncased\"\n","    max_length = 200\n","\n","    pretrained = True # for text encoder\n","    trainable = True # for text encoder\n","    temperature = 1.0\n","\n","    # image size\n","    size = 224\n","\n","    # for projection head; used for both image and text encoders\n","    num_projection_layers = 1\n","    projection_dim = 256 \n","    dropout = 0.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pXvTHzdfGp7r"},"source":["## Dataset & Data loader"]},{"cell_type":"code","metadata":{"id":"iULkHU0AGp7r"},"source":["class CLIPDataset(torch.utils.data.Dataset):\n","    def __init__(self, config, image_filenames, captions, tokenizer, transforms):\n","        \"\"\"\n","        image_filenames and cpations must have the same length; so, if there are\n","        multiple captions for each image, the image_filenames must have repetitive\n","        file names \n","        \"\"\"\n","        self.config = config\n","        self.image_filenames = image_filenames\n","        self.captions = list(captions)\n","        self.encoded_captions = tokenizer(\n","            list(captions), padding=True, truncation=True, max_length=config.max_length\n","        )\n","        self.transforms = transforms\n","\n","    def __getitem__(self, idx):\n","        item = {\n","            key: torch.tensor(values[idx])\n","            for key, values in self.encoded_captions.items()\n","        }\n","\n","        image = Image.open(f\"{self.config.image_path}/{self.image_filenames[idx]}\")\n","        image = self.transforms(image)\n","        item['image'] = image\n","        item['caption'] = self.captions[idx]\n","\n","        return item\n","\n","    def __len__(self):\n","        return len(self.captions)\n","    \n","\n","def build_loaders(config, dataframe, tokenizer, mode='train'):\n","    transforms = get_transforms(config)\n","    dataset = CLIPDataset(\n","        config,\n","        dataframe[\"image\"].values,\n","        dataframe[\"caption\"].values,\n","        tokenizer=tokenizer,\n","        transforms=transforms,\n","    )\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        shuffle=True if mode == \"train\" else False,\n","    )\n","    return dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T_tzm_fxGp7r"},"source":["## Define CLIP model"]},{"cell_type":"code","metadata":{"id":"W6eymq3CGp7s"},"source":["# you should implement \"Attention\" class in clip_modules.py to run following cells without error\n","from clip_modules import VisionTransformer, TextTransformer, ProjHead"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZL-gGGodGp7s"},"source":["def cross_entropy(preds, targets, reduction='none'):\n","    log_softmax = nn.LogSoftmax(dim=-1)\n","    loss = (-targets * log_softmax(preds)).sum(1)\n","    if reduction == \"none\":\n","        return loss\n","    elif reduction == \"mean\":\n","        return loss.mean()\n","    \n","\n","class CLIPModel(nn.Module):\n","    def __init__(\n","        self,\n","        config,\n","    ):\n","        super().__init__()\n","        self.image_encoder = VisionTransformer(image_size=224, patch_size=32, dim=2048, mlp_dim=2048, depth=6, dropout=0.1, emb_dropout=0.1, heads=16)\n","        self.text_encoder = TextTransformer(model_name=config.text_encoder_model)\n","        self.image_projection = ProjHead(embed_dim=config.image_embedding)\n","        self.text_projection = ProjHead(embed_dim=config.text_embedding)\n","        self.temperature = config.temperature\n","\n","    def forward(self, batch):\n","        # Getting Image and Text Features\n","        image_features = self.image_encoder(batch[\"image\"])\n","        text_features = self.text_encoder(\n","            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n","        )\n","        # Getting Image and Text Embeddings (with same dimension)\n","        image_embeddings = self.image_projection(image_features)\n","        text_embeddings = self.text_projection(text_features)\n","\n","        # Calculating the Loss\n","        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n","        images_similarity = image_embeddings @ image_embeddings.T\n","        texts_similarity = text_embeddings @ text_embeddings.T\n","        targets = F.softmax(\n","            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n","        )\n","        texts_loss = cross_entropy(logits, targets, reduction='none')\n","        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n","        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n","        return loss.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W9DktuGWGp7t"},"source":["## Training functions"]},{"cell_type":"code","metadata":{"id":"axBSp7DDGp7t"},"source":["def train_epoch(config, model, train_loader, optimizer, lr_scheduler, step):\n","    loss_meter = AvgMeter()\n","    tqdm_object = tqdm(train_loader, total=len(train_loader))\n","    for batch in tqdm_object:\n","        batch = {k: v.to(config.device) for k, v in batch.items() if k != \"caption\"}\n","        loss = model(batch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        if step == \"batch\":\n","            lr_scheduler.step()\n","\n","        count = batch[\"image\"].size(0)\n","        loss_meter.update(loss.item(), count)\n","\n","        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n","    return loss_meter\n","\n","\n","def valid_epoch(config, model, valid_loader):\n","    loss_meter = AvgMeter()\n","\n","    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n","    for batch in tqdm_object:\n","        batch = {k: v.to(config.device) for k, v in batch.items() if k != \"caption\"}\n","        loss = model(batch)\n","\n","        count = batch[\"image\"].size(0)\n","        loss_meter.update(loss.item(), count)\n","\n","        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n","    return loss_meter\n","\n","\n","def main(config):\n","    train_df, valid_df = make_train_valid_dfs(config)\n","    tokenizer = DistilBertTokenizer.from_pretrained(config.text_tokenizer)\n","    train_loader = build_loaders(config, train_df, tokenizer, mode='train')\n","    valid_loader = build_loaders(config, valid_df, tokenizer, mode='valid')\n","\n","    model = CLIPModel(config).to(config.device)    \n","    params = [\n","        {\"params\": model.image_encoder.parameters(), \"lr\": config.image_encoder_lr},\n","        {\"params\": model.text_encoder.parameters(), \"lr\": config.text_encoder_lr},\n","        {\"params\": itertools.chain(\n","            model.image_projection.parameters(), model.text_projection.parameters()\n","        ), \"lr\": config.head_lr, \"weight_decay\": config.weight_decay}\n","    ]\n","    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n","    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer, mode=\"min\", patience=config.patience, factor=config.factor\n","    )\n","    step = \"epoch\"\n","\n","    best_loss = float('inf')\n","    for epoch in range(config.epochs):\n","        print(f\"Epoch: {epoch + 1}\")\n","        model.train()\n","        train_loss = train_epoch(config, model, train_loader, optimizer, lr_scheduler, step)\n","        model.eval()\n","        with torch.no_grad():\n","            valid_loss = valid_epoch(config, model, valid_loader)\n","        \n","        if valid_loss.avg < best_loss:\n","            best_loss = valid_loss.avg\n","            torch.save(model.state_dict(), \"best.pt\")\n","            print(\"Saved Best Model!\")\n","        \n","        lr_scheduler.step(valid_loss.avg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrPX34AxGp7u"},"source":["main(CFG)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ES4HtCJ5Gp7u"},"source":["## Find matching images for the given query text"]},{"cell_type":"code","metadata":{"id":"8v9DTnWDGp7u"},"source":["def get_image_embeddings(config, valid_df, model_path):\n","    tokenizer = DistilBertTokenizer.from_pretrained(config.text_tokenizer)\n","    valid_loader = build_loaders(config, valid_df, tokenizer, mode=\"valid\")\n","    \n","    model = CLIPModel(config).to(config.device)\n","    model.load_state_dict(torch.load(model_path, map_location=config.device))\n","    model.eval()\n","    \n","    valid_image_embeddings = []\n","    with torch.no_grad():\n","        for batch in tqdm(valid_loader):\n","            image_features = model.image_encoder(batch[\"image\"].to(config.device))\n","            image_embeddings = model.image_projection(image_features)\n","            valid_image_embeddings.append(image_embeddings)\n","    return model, torch.cat(valid_image_embeddings)\n","\n","\n","def find_matches(config, model, image_embeddings, query, image_filenames, n=9):\n","    tokenizer = DistilBertTokenizer.from_pretrained(config.text_tokenizer)\n","    encoded_query = tokenizer([query])\n","    batch = {\n","        key: torch.tensor(values).to(config.device)\n","        for key, values in encoded_query.items()\n","    }\n","    with torch.no_grad():\n","        text_features = model.text_encoder(\n","            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n","        )\n","        text_embeddings = model.text_projection(text_features)\n","    \n","    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n","    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n","    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n","    \n","    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n","    matches = [image_filenames[idx] for idx in indices[::5]]\n","    matches = np.unique(matches)\n","    \n","    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n","    for match, ax in zip(matches, axes.flatten()):\n","        image = Image.open(f\"{config.image_path}/{match}\")\n","        ax.imshow(np.array(image))\n","        ax.axis(\"off\")\n","    \n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gkWfM8wPGp7v"},"source":["_, valid_df = make_train_valid_dfs(CFG)\n","model, image_embeddings = get_image_embeddings(CFG, valid_df, \"best.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IChVLt-0Gp7v"},"source":["find_matches(CFG, \n","             model, \n","             image_embeddings,\n","             query=\"a dog playing on the grass\",\n","             image_filenames=valid_df['image'].values,\n","             n=9)"],"execution_count":null,"outputs":[]}]}